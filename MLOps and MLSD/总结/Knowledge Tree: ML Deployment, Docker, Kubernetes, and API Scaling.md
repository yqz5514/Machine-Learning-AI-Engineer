```text
ğŸ“‚ Machine Learning Deployment
â”‚
â”œâ”€â”€ ğŸš€ Why Deploy an ML Model?
â”‚   â”œâ”€â”€ ğŸ”¹ Make ML models accessible to users and applications.
â”‚   â”œâ”€â”€ ğŸ”¹ Automate model inference (no need to run manually).
â”‚   â”œâ”€â”€ ğŸ”¹ Enable real-time predictions via API.
â”‚   â”œâ”€â”€ ğŸ”¹ Scale ML models to handle high traffic.
â”‚
â”œâ”€â”€ ğŸŒ ML API Deployment
â”‚   â”œâ”€â”€ ğŸ”¹ Converts ML models into **FastAPI or Flask** services.
â”‚   â”œâ”€â”€ ğŸ”¹ Users send JSON requests, API processes them, and returns predictions.
â”‚   â”œâ”€â”€ ğŸ“Œ Example:
â”‚   â”‚   â”œâ”€â”€ **Input:** `{"features": [5.1, 3.5, 1.4, 0.2]}`
â”‚   â”‚   â”œâ”€â”€ **ML Model Processes Data**
â”‚   â”‚   â”œâ”€â”€ **Output:** `{"prediction": 0}`
â”‚   â”œâ”€â”€ ğŸ”¹ API Deployment Methods:
â”‚   â”‚   â”œâ”€â”€ ğŸ–¥ï¸ **Local Machine** â€“ Runs API on your laptop (not accessible globally).
â”‚   â”‚   â”œâ”€â”€ â˜ï¸ **Cloud VM (AWS, GCP, Azure)** â€“ Deploys API on a cloud server.
â”‚   â”‚   â”œâ”€â”€ ğŸ“¦ **Docker Container** â€“ Packages API with dependencies for easy deployment.
â”‚   â”‚   â”œâ”€â”€ âš¡ **Serverless (AWS Lambda, Google Cloud Functions)** â€“ API runs **only when needed**.
â”‚
â”œâ”€â”€ ğŸ“¦ What is Docker?
â”‚   â”œâ”€â”€ ğŸ”¹ A containerization tool that **packages applications + dependencies**.
â”‚   â”œâ”€â”€ ğŸ”¹ Ensures the model runs **identically** on any machine.
â”‚   â”œâ”€â”€ ğŸ“Œ Key Components:
â”‚   â”‚   â”œâ”€â”€ **Docker Image** â€“ A blueprint of the application (like a frozen pizza ğŸ•).
â”‚   â”‚   â”œâ”€â”€ **Docker Container** â€“ A running instance of the image (a cooked pizza ğŸ•).
â”‚   â”‚   â”œâ”€â”€ **Dockerfile** â€“ A script defining how to build the image.
â”‚   â”œâ”€â”€ ğŸ“Œ Example:
â”‚   â”‚   â”œâ”€â”€ `docker build -t ml-api .` â†’ **Creates Docker Image**
â”‚   â”‚   â”œâ”€â”€ `docker run -p 5000:5000 ml-api` â†’ **Runs API inside a container**
â”‚
â”œâ”€â”€ â˜¸ï¸ What is Kubernetes?
â”‚   â”œâ”€â”€ ğŸ”¹ A tool for **orchestrating multiple Docker containers**.
â”‚   â”œâ”€â”€ ğŸ”¹ Manages **scalability, load balancing, and fault tolerance**.
â”‚   â”œâ”€â”€ ğŸ“Œ Key Components:
â”‚   â”‚   â”œâ”€â”€ **Pods** â€“ Smallest deployable unit (runs a single or multiple containers).
â”‚   â”‚   â”œâ”€â”€ **Deployments** â€“ Ensures the right number of containers are running.
â”‚   â”‚   â”œâ”€â”€ **Services** â€“ Makes containers accessible via a network.
â”‚   â”‚   â”œâ”€â”€ **Horizontal Pod Autoscaler (HPA)** â€“ Scales containers when CPU usage is high.
â”‚
â”œâ”€â”€ ğŸ”„ Why Scale ML APIs?
â”‚   â”œâ”€â”€ ğŸ”¹ Prevents **API slowdowns or crashes** when too many users send requests.
â”‚   â”œâ”€â”€ ğŸ”¹ Distributes workload **across multiple containers**.
â”‚   â”œâ”€â”€ ğŸ”¹ Ensures **high availability** (system keeps running even if one container fails).
â”‚
â”œâ”€â”€ ğŸ“ˆ Auto-Scaling ML API with Kubernetes
â”‚   â”œâ”€â”€ ğŸ”¹ Uses **Horizontal Pod Autoscaler (HPA)** to dynamically scale up/down containers.
â”‚   â”œâ”€â”€ ğŸ”¹ ğŸš€ Example of Scaling:
â”‚   â”‚   â”œâ”€â”€ **1:00 PM** â€“ 50 requests/sec â†’ **1 container handles it fine**.
â”‚   â”‚   â”œâ”€â”€ **1:10 PM** â€“ 200 requests/sec â†’ **Kubernetes adds 2 more containers**.
â”‚   â”‚   â”œâ”€â”€ **1:20 PM** â€“ 500 requests/sec â†’ **Scales to 5 containers to avoid crashes**.
â”‚   â”œâ”€â”€ ğŸ“Œ Kubernetes Scaling Commands:
â”‚   â”‚   â”œâ”€â”€ `kubectl apply -f k8s-deployment.yaml` â†’ Deploys ML API.
â”‚   â”‚   â”œâ”€â”€ `kubectl apply -f hpa.yaml` â†’ Enables auto-scaling.
â”‚   â”‚   â”œâ”€â”€ `kubectl get pods` â†’ Checks how many containers are running.
â”‚
â””â”€â”€ ğŸ”¥ Summary of Key Concepts
    â”œâ”€â”€ **ML Deployment:** Make models available via APIs.
    â”œâ”€â”€ **Docker:** Packages ML models into portable containers.
    â”œâ”€â”€ **Kubernetes:** Manages and scales ML containers efficiently.
    â”œâ”€â”€ **Scaling:** Prevents slow response times when traffic increases.
    â”œâ”€â”€ **Kubernetes Autoscaler:** Adds/removes containers based on demand.

```
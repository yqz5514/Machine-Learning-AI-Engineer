# ğŸ“Œ Designing Machine Learning Systems - Chapter 5.1â€“5.2: Feature Engineering (Part 1-missing value)

## ğŸ“Œ Chapter 5 Overview: What is Feature Engineering?
- **Definition**: Feature engineering is the process of selecting and transforming raw data into meaningful inputs that can be used by machine learning models.
- It determines **what information to use** and **how to represent it**.
- Effective feature engineering **improves model performance** and is often considered **more important than model choice**.

---

## ğŸ“Œ 5.1 Learned Features vs. Engineered Features

### **1ï¸âƒ£ Classical ML (Manual Feature Engineering)**
- Common in pre-deep learning workflows.
- Requires **domain knowledge** to manually:
  - Clean and normalize data.
  - Extract meaningful features (e.g., n-grams from text).
  - Encode and vectorize features.

ğŸ“Œ **Example**: Spam classification before deep learning:
- Manual preprocessing: lemmatization, removing stopwords/punctuation, expanding contractions.
- Creating n-grams (e.g., bigrams).
- Building a vocabulary and mapping text to feature vectors.

### **2ï¸âƒ£ Deep Learning (Learned Features)**
- Feature extraction is **partially or fully automated**.
- Steps typically include:
  - Tokenizing text.
  - Creating a vocabulary.
  - Mapping words to embeddings or one-hot vectors.
- The **neural network learns features automatically** from raw inputs (e.g., images or text).

ğŸ“Œ **Benefits**:
- **Reduces manual effort** and potential errors.
- **Scales better** to complex tasks.

ğŸ“Œ **Trade-offs**:
- Complex domains (e.g., fraud detection) may still **require expert-crafted features** to complement learned ones.

ğŸ“Œ **Real-World Example**:
- TikTok video recommendation may involve **millions of features**, requiring **automatic pipelines** and **feature selection systems**.

---

## ğŸ“Œ 5.2 Common Feature Engineering Operations (Start)

### **Handling Missing Values**
Missing values are **ubiquitous in real-world data**. But not all missing values are the same.

### **Types of Missing Data**
| Type | Description | Example |
|------|-------------|---------|
| **Missing Not at Random (MNAR)** | Missingness depends on the missing value itself. | People with **higher income** tend not to disclose it. |
| **Missing At Random (MAR)** | Missingness depends on another observed variable. | **Age** missing more often for **gender A** respondents. |
| **Missing Completely At Random (MCAR)** | No underlying reason for the missingness. | A job field left blank randomly. Rare in real-world data. |

---

### **Strategies for Handling Missing Values**

#### **1ï¸âƒ£ Deletion**

- **Column Deletion**:
  - If a feature has **too many missing values** (e.g., >= 50%, depends on requirements), drop it.

- **Row Deletion**:
  - If a row has missing values and MCAR condition holds, and **very few rows are affected** (e.g., <0.1%).

ğŸ“Œ **Risks of Deletion**:
- **Losing important signals**.
- **Bias amplification**:
  - Deleting MNAR data removes meaningful features.
  - Deleting MAR data (e.g., all rows missing age) may exclude specific subgroups entirely.

#### **2ï¸âƒ£ Imputation**

- Replace missing values with:
  - **Default values** (e.g., empty string).
  - **Mean / Median / Mode** of the feature.

ğŸ“Œ **Warnings**:
- Filling with possible values (e.g., fill number of children with 0) can confuse the model between **actual value** and **missingness**.
- Can cause **data leakage** or **bias injection**.
- Models may encounter **unknown imputed values during inference** if not seen during training.

ğŸ“Œ **Best Practices**:
- Always distinguish between "missing" and valid values (e.g., use an "unknown" flag).
- Evaluate imputation strategies through **experiments**, not assumptions.

---

## ğŸ“Œ Final Takeaways (for Part 1)
1ï¸âƒ£ **Deep learning has automated much of feature extraction**, but classical feature engineering is still vital in many domains.  
2ï¸âƒ£ **Missing values are common and nuanced**â€”different types of missingness require different strategies.  
3ï¸âƒ£ **Imputation and deletion come with trade-offs**, and the right choice depends on the data distribution and model requirements.  

---
# ğŸ“Œ Designing Machine Learning Systems - Chapter 5.2 (b): Feature Engineering â€“ Scaling

## ğŸ“Œ Why Feature Scaling Matters

- **Feature scaling** ensures input features are on similar ranges.
- Especially important for:
  - **Distance-based models** (e.g., k-NN, SVM).
  - **Gradient-based models** (e.g., logistic regression, gradient boosting).
- **Neglecting to scale** can result in:
  - Gibberish predictions.
  - Slower or unstable convergence.
  - Models being dominated by features with larger ranges.

ğŸ“Œ **Key Insight**: Scaling is **simple but critical**â€”especially in traditional ML pipelines.

---

## ğŸ“Œ Scaling Techniques

### **1ï¸âƒ£ Min-Max Scaling (Normalization)**

- **Rescales values to a specific range**, commonly \([0, 1]\) or \([-1, 1]\).

#### **Formula**:
For range \([0, 1]\):
\[
x' = \frac{x - \min(x)}{\max(x) - \min(x)}
\]

For range \([a, b]\) (e.g., \([-1, 1]\)):
\[
x' = a + \frac{(x - \min(x))(b - a)}{\max(x) - \min(x)}
\]

ğŸ“Œ **Tip**: \([-1, 1]\) often performs better empirically than \([0, 1]\).

---

### **2ï¸âƒ£ Standardization (Z-score Normalization)**

- Use when data is **approximately normally distributed** or when you **donâ€™t want to assume bounds**.

#### **Formula**:
\[
x' = \frac{x - \mu}{\sigma}
\]
- Where:
  - \(\mu\) is the **mean** of the feature.
  - \(\sigma\) is the **standard deviation**.

ğŸ“Œ **Benefit**: Results in features with **mean = 0** and **std = 1**, which can improve performance in models that assume normality.

---

### **3ï¸âƒ£ Log Transformation**

- Helps with **skewed distributions** (e.g., income, counts, durations).
- **Transforms data** by applying:
  \[
  x' = \log(x + 1)
  \]
  (adding 1 avoids \(\log(0)\) issues).

ğŸ“Œ **Use Cases**:
- Works well for **positive-only features**.
- Common in domains like **finance, health, fraud detection**.

ğŸ“Œ **Caution**:
- Doesnâ€™t work for all features.
- Can affect **interpretability and downstream analysis**.

---

## ğŸ“Œ Additional Considerations

### **1ï¸âƒ£ Scaling and Data Leakage**
- **Scaling uses global statistics (min, max, mean, std)** from training data.
- **DO NOT** use test data to compute scaling parametersâ€”this leads to **data leakage**.
- During inference, **reuse training stats** to transform new samples.

### **2ï¸âƒ£ Scaling Drift**
- **If new incoming data distribution shifts**, original stats become stale.
- **Retrain or update scaling parameters periodically** to maintain performance.

---

## âœ… Summary: When and How to Scale Features

| **Technique** | **When to Use** | **Pros** | **Cons** |
|---------------|----------------|----------|----------|
| Min-Max Scaling | When feature bounds are known and fixed. | Simple, interpretable | Sensitive to outliers |
| Standardization | When features follow (or assumed to follow) a normal distribution. | Robust, widely supported | Affected by outliers |
| Log Transformation | When data is skewed (e.g., exponential). | Reduces skewness, improves symmetry | Only for positive values, not always effective |

ğŸ“Œ **Key Takeaway**: Always evaluate scaling decisions via experimentsâ€”scaling may be simple, but its impact on model performance is significant.

---

## ğŸ“Œ Final Thoughts

1ï¸âƒ£ **Scaling is essential for gradient-based and distance-based models**â€”never skip it in traditional ML.  
2ï¸âƒ£ **Standardization and min-max scaling are go-to choices**, depending on your data shape.  
3ï¸âƒ£ **Be aware of data leakage and drift when applying scaling in production pipelines.**

---

# ğŸ“Œ Designing Machine Learning Systems - Chapter 5.2 (c): Discretization

### **This technique is included in this book for completeness, though in practice, Iâ€™ve rarely found discretization to help.**

## ğŸ“Œ What is Discretization?

- **Discretization** (also called **quantization** or **binning**) is the process of converting a **continuous variable into discrete buckets**.
- It's typically used to **simplify learning**, especially when:
  - You have **limited training data**.
  - The model struggles to generalize over **small numeric differences**.

ğŸ“Œ **Example**:  
If the model has seen incomes like `$150,000`, `$50,000`, `$100,000` during training, and sees `$9,000.50` during inference, it might fail to understand that `$9,000.50` is closer to `$10,000` than `$150,000`.

---

## ğŸ“Œ Why Use Discretization?

- Models **donâ€™t inherently understand numeric similarity** (e.g., $9,000.50 â‰  $10,000 unless encoded or transformed).
- Grouping values into **coarse categories** reduces the model's burden of learning from infinitely many values.
- More useful when **training data is small or sparse**.

ğŸ“Œ **Income Grouping Example**:
| Bucket | Rule |
|--------|------|
| Low Income | < $35,000/year |
| Middle Income | $35,000â€“$100,000/year |
| High Income | > $100,000/year |

---

## ğŸ“Œ Discretization Can Also Apply to Discrete Features

- Although designed for continuous features, **discretization can also apply to discrete features**.
- **Example â€“ Age Buckets**:
  - < 18  
  - 18â€“22  
  - 22â€“30  
  - 30â€“40  
  - 40â€“65  
  - > 65  

ğŸ“Œ **Why?**  
To reduce feature complexity or align with **real-world semantics** (e.g., age groups in demographic models).

---

## âš ï¸ Drawbacks of Discretization

1. **Information Loss at Boundaries**:
   - `$34,999` and `$35,000` end up in different bucketsâ€”even though they are only $1 apart.
   - Conversely, `$35,000` and `$100,000` may be in the same bucketâ€”though far apart.

2. **Choosing Cutoffs is Hard**:
   - Requires **manual design** or domain expertise.
   - May introduce **bias** or reduce model granularity.
   - Use **histograms** and **quantiles** to guide boundaries.

3. **Discontinuity**:
   - Model sees **sudden jumps in input values**, which may hurt smooth decision boundaries.

---

## âœ… Best Practices

- **Use with caution**â€”only if:
  - The model is simple.
  - Training data is small.
  - Interpretability is prioritized.
- **Visualize distributions** to inform cutoff choices.
- Consider **automated binning strategies**:
  - Equal-width bins.
  - Equal-frequency bins.
  - Decision-treeâ€“based binning.

---

## ğŸ“Œ Final Takeaways (3-Sentence Summary)

1ï¸âƒ£ **Discretization converts continuous features into buckets to simplify model learning**, especially in low-data regimes.  
2ï¸âƒ£ **Though useful, it introduces discontinuities and may obscure important variations in input values**.  
3ï¸âƒ£ **Use visualization, quantiles, or domain knowledge to set meaningful boundariesâ€”and apply it thoughtfully**.

---

# ğŸ“Œ Designing Machine Learning Systems - Chapter 5.2 (d): Encoding Categorical Features

## ğŸ“Œ Why Encoding Categorical Features Matters

- **Categorical features** must be transformed into **numerical representations** to be used in ML models.
- Many beginners assume categories are **static**, but in production, **categories evolve over time**:
  - New product brands, users, IPs, restaurants, domains, etc. appear frequently.
  - Models must **generalize to unseen categories** or risk failing in production.

ğŸ“Œ **Key Insight**: A robust encoding strategy must handle both **known** and **new/unseen** categories.

---

## ğŸ“Œ Common Pitfalls in Categorical Encoding

### **1ï¸âƒ£ Static Encoding**
- Assigning **each unique category a unique ID** (e.g., Brand A â†’ 0, Brand B â†’ 1, â€¦).
- **Fails in production** when:
  - New categories appear that the model hasnâ€™t seen.
  - Unseen categories cause **model crashes or degraded performance**.

### **Real-World Example: Amazon**
- Over 2 million brands exist.
- You encode brands from `0` to `1,999,999`, but when a **new brand appears**, the model fails.
- Fix: Add an **`UNKNOWN` category** to catch all unseen brands.

---

## ğŸ“Œ Problems with the UNKNOWN Category

- If `UNKNOWN` is **not seen during training**, the model wonâ€™t learn how to treat it.
- Sellers complain their **new products donâ€™t get recommended**.
- You update the strategy:
  - Encode **top 99% most common brands**.
  - Encode **bottom 1% + all unseen brands** as `UNKNOWN`.

ğŸ“Œ **Still Not Enough**:
- New brands (some important) are lumped together as "UNKNOWN".
- Model treats **popular new brands** like **irrelevant or spammy ones**.

---

## ğŸ“Œ Generalization Issues with Static Binning

- The issue is **not unique to Amazon**:
  - Spam detection: new users, new websites.
  - E-commerce: new product categories.
  - Any feature involving dynamic entities (e.g., IPs, accounts, URLs).

ğŸ“Œ **Challenge**: How do you meaningfully represent **new categories** without re-training your model?

---

## ğŸ“Œ The Hashing Trick

- **Solution**: Use a **hash function** to encode categories into fixed-size buckets.
- Popularized by **Vowpal Wabbit**, also used in **scikit-learn**, **TensorFlow**, **Gensim**.

### **How it works**:
1. Apply a hash function to the category name.
2. Use the **modulo** operation to map the hash to a fixed integer range.
3. All categories (even new ones) are **guaranteed to be mapped** to a numeric index.

ğŸ“Œ **Example**:
- Choose a hash space of `2^18 = 262,144`.
- Any new or unseen category gets mapped to an index in `[0, 262,143]`.

---

## ğŸ“Œ Pros and Cons of Hashing

| **Pros** | **Cons** |
|----------|----------|
| Handles unseen categories gracefully. | **Hash collisions**: Two categories may map to the same index. |
| Avoids model crashes in production. | Loss of interpretability. |
| Works well in online learning. | Collisions can slightly degrade performance. |

ğŸ“Œ **Good News**:
- Research (e.g., Booking.com) shows that even with **50% collisions**, the **performance drop is < 0.5%**.
- Can use **locality-sensitive hashing** if similarity preservation is important.

---

## ğŸ“Œ Best Practices

- Use **hashing** for features with:
  - A **large number of unique categories**.
  - **High turnover rate** (e.g., accounts, URLs).
- Keep hash space **large enough** to minimize collisions.
- Monitor for **drift in category distribution** over time.

---

## ğŸ“Œ Final Takeaways (3-Sentence Summary)

1ï¸âƒ£ **Production ML systems must handle unseen categorical values**, which static encodings cannot reliably support.  
2ï¸âƒ£ The **hashing trick** offers a scalable solution by mapping all categories (even new ones) into a fixed feature space.  
3ï¸âƒ£ Though it introduces some collisions, hashing performs well in practice and is widely used in modern ML pipelines.

---
